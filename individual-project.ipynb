{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output of LLMs with Rewriting Prompt - Gemini and Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Overview\n",
    "**LLM Prompt Recovery Data for Gemini and Gemma**: This is a dataset containing text data from gemini and gemma models. Each of them contains original text, a prompt to require rewriting the text in a specified direction, and the rewritten text.  \n",
    "https://www.kaggle.com/datasets/newtonbaba12345/llm-prompt-recovery-data-gemini-and-gemma.   \n",
    "\n",
    "### Project Goal\n",
    "My idea is to analyze the relationship between original text and rewritten text, prompt and rewritten text, as well as (original text + prompt) and rewritten text. I use cosine similarity to calculate semantic similarity for each of the three pairs, and do some comparison.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install / Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-30T03:54:00.065723Z",
     "iopub.status.busy": "2024-04-30T03:54:00.065303Z",
     "iopub.status.idle": "2024-04-30T03:54:00.141589Z",
     "shell.execute_reply": "2024-04-30T03:54:00.140175Z",
     "shell.execute_reply.started": "2024-04-30T03:54:00.065688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/experts-bert/tensorflow2/wiki-books-qqp/2/saved_model.pb\n",
      "/kaggle/input/experts-bert/tensorflow2/wiki-books-qqp/2/assets/vocab.txt\n",
      "/kaggle/input/experts-bert/tensorflow2/wiki-books-qqp/2/variables/variables.index\n",
      "/kaggle/input/experts-bert/tensorflow2/wiki-books-qqp/2/variables/variables.data-00000-of-00001\n",
      "/kaggle/input/experts-bert/tensorflow2/pubmed/2/saved_model.pb\n",
      "/kaggle/input/experts-bert/tensorflow2/pubmed/2/assets/vocab.txt\n",
      "/kaggle/input/experts-bert/tensorflow2/pubmed/2/variables/variables.index\n",
      "/kaggle/input/experts-bert/tensorflow2/pubmed/2/variables/variables.data-00000-of-00001\n",
      "/kaggle/input/llm-prompt-recovery-data-gemini-and-gemma/gemma_data_set_prompt_recover_1.csv\n",
      "/kaggle/input/llm-prompt-recovery-data-gemini-and-gemma/gemma_data_set_prompt_recover_2.csv\n",
      "/kaggle/input/llm-prompt-recovery-data-gemini-and-gemma/gemini_data_set_prompt_recover_3.csv\n",
      "/kaggle/input/bert/tensorflow2/en-uncased-preprocess/3/saved_model.pb\n",
      "/kaggle/input/bert/tensorflow2/en-uncased-preprocess/3/keras_metadata.pb\n",
      "/kaggle/input/bert/tensorflow2/en-uncased-preprocess/3/assets/vocab.txt\n",
      "/kaggle/input/bert/tensorflow2/en-uncased-preprocess/3/variables/variables.index\n",
      "/kaggle/input/bert/tensorflow2/en-uncased-preprocess/3/variables/variables.data-00000-of-00001\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "\n",
    "# I've checked that we can appropriately ignore some warnings here\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T03:54:04.430699Z",
     "iopub.status.busy": "2024-04-30T03:54:04.430310Z",
     "iopub.status.idle": "2024-04-30T03:54:04.436632Z",
     "shell.execute_reply": "2024-04-30T03:54:04.435455Z",
     "shell.execute_reply.started": "2024-04-30T03:54:04.430669Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) # to display full content in a row "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Data\n",
    "There are two files for the gemma model, and one file is already enough for me to do the analysis, so I read in its first file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:03:37.640367Z",
     "iopub.status.busy": "2024-04-30T04:03:37.639174Z",
     "iopub.status.idle": "2024-04-30T04:03:37.722440Z",
     "shell.execute_reply": "2024-04-30T04:03:37.721511Z",
     "shell.execute_reply.started": "2024-04-30T04:03:37.640288Z"
    }
   },
   "outputs": [],
   "source": [
    "gemini = pd.read_csv('/kaggle/input/llm-prompt-recovery-data-gemini-and-gemma/gemini_data_set_prompt_recover_3.csv')\n",
    "gemma = pd.read_csv('/kaggle/input/llm-prompt-recovery-data-gemini-and-gemma/gemma_data_set_prompt_recover_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T03:55:32.620875Z",
     "iopub.status.busy": "2024-04-30T03:55:32.620488Z",
     "iopub.status.idle": "2024-04-30T03:55:32.628737Z",
     "shell.execute_reply": "2024-04-30T03:55:32.627427Z",
     "shell.execute_reply.started": "2024-04-30T03:55:32.620844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Model_used', 'original_text', 'prompt', 'rewritten_text'], dtype='object')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check columns \n",
    "gemini.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T03:55:34.287636Z",
     "iopub.status.busy": "2024-04-30T03:55:34.286661Z",
     "iopub.status.idle": "2024-04-30T03:55:34.294635Z",
     "shell.execute_reply": "2024-04-30T03:55:34.293425Z",
     "shell.execute_reply.started": "2024-04-30T03:55:34.287595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_text', 'prompt', 'rewrite_prompt', 'rewritten_text'], dtype='object')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check columns \n",
    "gemma.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T02:40:46.053207Z",
     "iopub.status.busy": "2024-04-30T02:40:46.052562Z",
     "iopub.status.idle": "2024-04-30T02:40:46.059938Z",
     "shell.execute_reply": "2024-04-30T02:40:46.058498Z",
     "shell.execute_reply.started": "2024-04-30T02:40:46.053173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1802, 4)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dimension \n",
    "gemini.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T02:40:47.372825Z",
     "iopub.status.busy": "2024-04-30T02:40:47.372453Z",
     "iopub.status.idle": "2024-04-30T02:40:47.381050Z",
     "shell.execute_reply": "2024-04-30T02:40:47.379386Z",
     "shell.execute_reply.started": "2024-04-30T02:40:47.372797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dimension \n",
    "gemma.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the content to corresponding variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:03:43.221062Z",
     "iopub.status.busy": "2024-04-30T04:03:43.220672Z",
     "iopub.status.idle": "2024-04-30T04:03:43.228490Z",
     "shell.execute_reply": "2024-04-30T04:03:43.226761Z",
     "shell.execute_reply.started": "2024-04-30T04:03:43.221025Z"
    }
   },
   "outputs": [],
   "source": [
    "# gemini\n",
    "\n",
    "org_text = gemini.original_text # original text, every 5 rows are the same inputs \n",
    "prompt = gemini.prompt # required adjustments, each is different \n",
    "rew_text = gemini.rewritten_text # rewritten text, each is different  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:03:44.638627Z",
     "iopub.status.busy": "2024-04-30T04:03:44.638245Z",
     "iopub.status.idle": "2024-04-30T04:03:44.644753Z",
     "shell.execute_reply": "2024-04-30T04:03:44.643487Z",
     "shell.execute_reply.started": "2024-04-30T04:03:44.638600Z"
    }
   },
   "outputs": [],
   "source": [
    "# gemma\n",
    "\n",
    "org_text_gemma = gemma.original_text # original text each is different \n",
    "prompt_gemma = gemma.prompt # required adjustments, each is different \n",
    "rew_text_gemma = gemma.rewritten_text # rewritten text, each is different  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process org_text, save as array in **org_text_list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:03:46.496895Z",
     "iopub.status.busy": "2024-04-30T04:03:46.496403Z",
     "iopub.status.idle": "2024-04-30T04:03:46.854956Z",
     "shell.execute_reply": "2024-04-30T04:03:46.853998Z",
     "shell.execute_reply.started": "2024-04-30T04:03:46.496849Z"
    }
   },
   "outputs": [],
   "source": [
    "# gemini\n",
    "\n",
    "org_text_list = [[] for _ in range(len(rew_text))]\n",
    "\n",
    "for i in range(len(org_text)): \n",
    "    org_text[i] = org_text[i].replace(\"\\n\", \" \")\n",
    "    org_text[i] = org_text[i].replace(\"\\\\n\", \" \")\n",
    "    org_text_list[i].append(org_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:03:48.303883Z",
     "iopub.status.busy": "2024-04-30T04:03:48.303517Z",
     "iopub.status.idle": "2024-04-30T04:03:48.710968Z",
     "shell.execute_reply": "2024-04-30T04:03:48.709834Z",
     "shell.execute_reply.started": "2024-04-30T04:03:48.303854Z"
    }
   },
   "outputs": [],
   "source": [
    "# gemma\n",
    "\n",
    "org_text_list_gemma = [[] for _ in range(len(rew_text_gemma))]\n",
    "\n",
    "for i in range(len(org_text_gemma)): \n",
    "    org_text_gemma[i] = org_text_gemma[i].replace(\"\\n\", \" \")\n",
    "    org_text_gemma[i] = org_text_gemma[i].replace(\"`` \", \"\")\n",
    "    org_text_gemma[i] = org_text_gemma[i].replace(\" ``\", \"\")\n",
    "    org_text_gemma[i] = org_text_gemma[i].replace(\"*\", \"\")\n",
    "    org_text_list_gemma[i].append(org_text_gemma[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:03:55.373457Z",
     "iopub.status.busy": "2024-04-30T04:03:55.373069Z",
     "iopub.status.idle": "2024-04-30T04:03:55.380476Z",
     "shell.execute_reply": "2024-04-30T04:03:55.379114Z",
     "shell.execute_reply.started": "2024-04-30T04:03:55.373427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Memo  To: All Staff From: Keith Subject: Fish Tank Maintenance  Please be reminded to clean the fish tanks in the office every Friday. This ensures a healthy environment for our beloved fish. Thank you for your cooperation.']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly check \n",
    "org_text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:03:56.988368Z",
     "iopub.status.busy": "2024-04-30T04:03:56.987857Z",
     "iopub.status.idle": "2024-04-30T04:03:56.997285Z",
     "shell.execute_reply": "2024-04-30T04:03:56.995653Z",
     "shell.execute_reply.started": "2024-04-30T04:03:56.988308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It'd been three days since Baldr Alwisson had seen signs of pursuit, yet he could n't stop. The damned snow had stopped falling, leaving his tracks painfully clear. Between that and the hounds, the soldiers could n't fail to catch him eventually. What he truly needed was a blizzard, but the pale skies offered not the promise of such.     It was his own fault, in truth. He should n't have grabbed the jeweled plaque from that temple, gold inlay and sapphires or no. The priests and parishioners were out for his life, he knew because the plaque told him. It was now giving Baldr directions to thwart those following. Somewhere up ahead, there was a cave that was always warm inside, a cave the army could n't hope to find him in.    ... stuck. Thought I had something for a second there, it escaped. \"]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly check \n",
    "org_text_list_gemma[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine org_text and prompt as the X for regression, save as array in **X_list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:11.589187Z",
     "iopub.status.busy": "2024-04-30T04:04:11.588781Z",
     "iopub.status.idle": "2024-04-30T04:04:11.625397Z",
     "shell.execute_reply": "2024-04-30T04:04:11.623639Z",
     "shell.execute_reply.started": "2024-04-30T04:04:11.589155Z"
    }
   },
   "outputs": [],
   "source": [
    "# gemini\n",
    "X_list = []\n",
    "\n",
    "for i in range(len(prompt)): \n",
    "    prompt_string = str(prompt[i]).split('. ')[1: ]\n",
    "    X = org_text[i] + str(prompt_string) # combine org_text and prompt as the X for regression\n",
    "    X = X.replace(\"['\", \" \")\n",
    "    X = X.replace(\"']\", \"\")\n",
    "    X = X.replace(\"\\n\", \"\")\n",
    "    X = np.array(X)\n",
    "    X_list.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:15.978114Z",
     "iopub.status.busy": "2024-04-30T04:04:15.977652Z",
     "iopub.status.idle": "2024-04-30T04:04:16.003877Z",
     "shell.execute_reply": "2024-04-30T04:04:16.001466Z",
     "shell.execute_reply.started": "2024-04-30T04:04:15.978082Z"
    }
   },
   "outputs": [],
   "source": [
    "# gemma\n",
    "X_list_gemma = []\n",
    "\n",
    "for i in range(len(prompt_gemma)): \n",
    "    prompt_string = prompt_gemma[i]\n",
    "    X = org_text_gemma[i] + str(prompt_string) # combine org_text and prompt as the X for regression\n",
    "    X = X.replace(\"['\", \" \")\n",
    "    X = X.replace(\"']\", \"\")\n",
    "    X = X.replace(\"\\n\", \"\")\n",
    "    X = X.replace(\"`` \", \"\")\n",
    "    X = np.array(X)\n",
    "    X_list_gemma.append(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:18.608857Z",
     "iopub.status.busy": "2024-04-30T04:04:18.608479Z",
     "iopub.status.idle": "2024-04-30T04:04:18.617132Z",
     "shell.execute_reply": "2024-04-30T04:04:18.615515Z",
     "shell.execute_reply.started": "2024-04-30T04:04:18.608828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('Hey Suzann! Just wanted to let you know that Grandma and Grandpa are coming over for dinner tonight. Can you pick up some dessert on your way home? Thanks! Rewrite the text to make it sound less demanding.',\n",
       "      dtype='<U205')"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly check \n",
    "X_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:20.405773Z",
     "iopub.status.busy": "2024-04-30T04:04:20.405412Z",
     "iopub.status.idle": "2024-04-30T04:04:20.412903Z",
     "shell.execute_reply": "2024-04-30T04:04:20.411623Z",
     "shell.execute_reply.started": "2024-04-30T04:04:20.405747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(\"It'd been three days since Baldr Alwisson had seen signs of pursuit, yet he could n't stop. The damned snow had stopped falling, leaving his tracks painfully clear. Between that and the hounds, the soldiers could n't fail to catch him eventually. What he truly needed was a blizzard, but the pale skies offered not the promise of such.     It was his own fault, in truth. He should n't have grabbed the jeweled plaque from that temple, gold inlay and sapphires or no. The priests and parishioners were out for his life, he knew because the plaque told him. It was now giving Baldr directions to thwart those following. Somewhere up ahead, there was a cave that was always warm inside, a cave the army could n't hope to find him in.    ... stuck. Thought I had something for a second there, it escaped. Rewrite this text in the style of a Victorian gentleman futuristic AI.\",\n",
       "      dtype='<U872')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly check \n",
    "X_list_gemma[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process prompt, save as array in **prompt_list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:33.535105Z",
     "iopub.status.busy": "2024-04-30T04:04:33.534720Z",
     "iopub.status.idle": "2024-04-30T04:04:33.548481Z",
     "shell.execute_reply": "2024-04-30T04:04:33.547583Z",
     "shell.execute_reply.started": "2024-04-30T04:04:33.535074Z"
    }
   },
   "outputs": [],
   "source": [
    "# gemini\n",
    "prompt_list = []\n",
    "\n",
    "for i in range(len(prompt)): \n",
    "    prompt_string = str(prompt[i]).split('. ')[1: ]\n",
    "    prompt_list.append(prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:36.157261Z",
     "iopub.status.busy": "2024-04-30T04:04:36.156641Z",
     "iopub.status.idle": "2024-04-30T04:04:36.167801Z",
     "shell.execute_reply": "2024-04-30T04:04:36.166838Z",
     "shell.execute_reply.started": "2024-04-30T04:04:36.157174Z"
    }
   },
   "outputs": [],
   "source": [
    "#gemma \n",
    "prompt_list_gemma = []\n",
    "\n",
    "for i in range(len(prompt_gemma)): \n",
    "    prompt_string = prompt_gemma[i]\n",
    "    prompt_list_gemma.append(prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:40.925260Z",
     "iopub.status.busy": "2024-04-30T04:04:40.924255Z",
     "iopub.status.idle": "2024-04-30T04:04:40.931131Z",
     "shell.execute_reply": "2024-04-30T04:04:40.930056Z",
     "shell.execute_reply.started": "2024-04-30T04:04:40.925208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rewrite this paragraph to make it more engaging and fun.']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly check \n",
    "prompt_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:42.587202Z",
     "iopub.status.busy": "2024-04-30T04:04:42.586500Z",
     "iopub.status.idle": "2024-04-30T04:04:42.594207Z",
     "shell.execute_reply": "2024-04-30T04:04:42.593081Z",
     "shell.execute_reply.started": "2024-04-30T04:04:42.587146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate the essence of this text into a the Roaring Twenties narrative.'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly check \n",
    "prompt_list_gemma[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process rew_text, save as array in **rew_text_list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:45.628362Z",
     "iopub.status.busy": "2024-04-30T04:04:45.627595Z",
     "iopub.status.idle": "2024-04-30T04:04:45.859746Z",
     "shell.execute_reply": "2024-04-30T04:04:45.858304Z",
     "shell.execute_reply.started": "2024-04-30T04:04:45.628296Z"
    }
   },
   "outputs": [],
   "source": [
    "# gemini\n",
    "rew_text_list = [[] for _ in range(len(rew_text))]\n",
    "\n",
    "for i in range(len(rew_text)): \n",
    "    rew_text[i] = rew_text[i].replace(\"\\n\", \"\")\n",
    "    rew_text_list[i].append(rew_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:52.592208Z",
     "iopub.status.busy": "2024-04-30T04:04:52.591821Z",
     "iopub.status.idle": "2024-04-30T04:04:52.889933Z",
     "shell.execute_reply": "2024-04-30T04:04:52.888895Z",
     "shell.execute_reply.started": "2024-04-30T04:04:52.592179Z"
    }
   },
   "outputs": [],
   "source": [
    "#gemma\n",
    "rew_text_list_gemma = [[] for _ in range(len(rew_text_gemma))]\n",
    "\n",
    "for i in range(len(rew_text_gemma)): \n",
    "    rew_text_gemma[i] = rew_text_gemma[i].replace(\"\\n\", \"\")\n",
    "    rew_text_gemma[i] = rew_text_gemma[i].replace(\"\\'\", \"'\")\n",
    "    rew_text_gemma[i] = rew_text_gemma[i].replace('*', \"\")\n",
    "    rew_text_list_gemma[i].append(rew_text_gemma[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:54.949251Z",
     "iopub.status.busy": "2024-04-30T04:04:54.948875Z",
     "iopub.status.idle": "2024-04-30T04:04:54.956382Z",
     "shell.execute_reply": "2024-04-30T04:04:54.955193Z",
     "shell.execute_reply.started": "2024-04-30T04:04:54.949223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hi Suzann,Just wanted to ask you a favor. My grandparents are visiting for dinner tonight, and it would be great if you could help me with dessert. If you could pick something up on your way home, I'd really appreciate it. Thanks!\"]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly check \n",
    "rew_text_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:04:56.432387Z",
     "iopub.status.busy": "2024-04-30T04:04:56.431991Z",
     "iopub.status.idle": "2024-04-30T04:04:56.440526Z",
     "shell.execute_reply": "2024-04-30T04:04:56.439052Z",
     "shell.execute_reply.started": "2024-04-30T04:04:56.432356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here is the text rewritten in the style of a Victorian gentleman futuristic AI:\"My dear Baldr Alwisson, it has been three days since I have encountered the infernal signs of pursuit, yet I am, alas, unable to desist. The wretched snow has ceased its reign of fall, leaving my tracks bare and discernible. Between this infernal precipitation and the hounds of war, your capture is inevitable. What I truly require, my dear Baldr, is a blizzard, a tempest to mask my presence, but the pale heavens offer not the promise of such a windstorm.It is an admission of fault, I must say, to have partaken of the jeweled plaque from that temple of antiquity, adorned with gold inlay and sapphires. The priests and parishioners are clearly out for my blood, their wrath imminent. I know their intentions well, for the plaque bestowed upon me has revealed the path I must traverse in order to outsmart and out']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly check \n",
    "rew_text_list_gemma[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 lists are prepared: \n",
    "Until here, I have prepared org_text_list, X_list, prompt_list, rew_text_list, for gemini and gemma, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pretrained model: google/experts-bert\n",
    "This is a collection of such BERT \"expert\" models that were trained on a diversity of datasets and tasks to improve performance on downstream tasks like question answering, tasks that require natural language inference skills, NLP tasks in the medical text domain, and more.  \n",
    "\n",
    "There are different specialties for different versions. For calculating semantic similarity, I use the wiki_books/qqp version:  \n",
    "This model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on the Quora Question Pairs dataset (QQP), a dataset for the semantic similarity of question pairs.  \n",
    "\n",
    "The reason I did semantic similarity analysis is that, I suppose that if **the semantic similarity between original text combined with prompt and rewritten text** is higher than **the semantic similarity between original text and rewritten text**, it indicates that the semantic meaning of prompt is well captured and added in the rewritten text. This could mean the model performs well. Although very simple, I want to investigate the performance of models through this way and see what I can find.  \n",
    "\n",
    "More information can be found at https://www.kaggle.com/models/google/experts-bert/tensorFlow2/wiki-books-qqp.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:05:06.271360Z",
     "iopub.status.busy": "2024-04-30T04:05:06.270951Z",
     "iopub.status.idle": "2024-04-30T04:05:16.826130Z",
     "shell.execute_reply": "2024-04-30T04:05:16.825262Z",
     "shell.execute_reply.started": "2024-04-30T04:05:06.271314Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching model 'tensorflow/bert/tensorflow2/en-uncased-preprocess/3' to your Kaggle notebook...\n",
      "Attaching model 'google/experts-bert/tensorflow2/wiki-books-qqp/2' to your Kaggle notebook...\n"
     ]
    }
   ],
   "source": [
    "# !pip install --quiet tensorflow-text\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text  # Imports TF ops for preprocessing\n",
    "\n",
    "# compute cosine similarity\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "# load the BERT encoder and preprocessing models\n",
    "preprocess = hub.load('https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3')\n",
    "bert_qqp = hub.load('https://www.kaggle.com/models/google/experts-bert/TensorFlow2/wiki-books-qqp/2')\n",
    "# /qqp: this model was initialized from the base Wikipedia + BooksCorpus BERT model and was fine-tuned on the Quora Question Pairs dataset (QQP), \n",
    "# a dataset for the semantic similarity of question pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:05:49.579271Z",
     "iopub.status.busy": "2024-04-30T04:05:49.578882Z",
     "iopub.status.idle": "2024-04-30T04:05:49.585300Z",
     "shell.execute_reply": "2024-04-30T04:05:49.583586Z",
     "shell.execute_reply.started": "2024-04-30T04:05:49.579244Z"
    }
   },
   "outputs": [],
   "source": [
    "# since the number of rows of gemini and gemma are different, define a number of iteration  \n",
    "num = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Embeddings\n",
    "For gemini and gemma, I construct two loops to iterate through `num` rows of data, respectively, and simultaneously calculate the cosine similarity between lists.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:05:56.921229Z",
     "iopub.status.busy": "2024-04-30T04:05:56.920821Z",
     "iopub.status.idle": "2024-04-30T04:22:34.936428Z",
     "shell.execute_reply": "2024-04-30T04:22:34.935432Z",
     "shell.execute_reply.started": "2024-04-30T04:05:56.921198Z"
    }
   },
   "outputs": [],
   "source": [
    "# gemini\n",
    "\n",
    "# create lists to record output embedding\n",
    "pooled_output_org_list = []\n",
    "sequence_output_org_list = []\n",
    "\n",
    "pooled_output_prompt_list = []\n",
    "sequence_output_prompt_list = []\n",
    "\n",
    "pooled_output_X_list = []\n",
    "sequence_output_X_list = []\n",
    "\n",
    "pooled_output_rew_list = []\n",
    "sequence_output_rew_list = []\n",
    "\n",
    "# lists to record similarity between lists and rewritten \n",
    "cos_sim_list_org_rew = []\n",
    "cos_sim_list_prompt_rew = []\n",
    "cos_sim_list_X_rew = []\n",
    "\n",
    "\n",
    "for i in range(num): \n",
    "    # ======================================\n",
    "    # org \n",
    "    org_sentences = org_text_list[i]\n",
    "    \n",
    "    # convert the sentences to bert inputs\n",
    "    bert_inputs_org = preprocess(org_sentences)\n",
    "    \n",
    "    # feed the inputs to the model to get the pooled and sequence outputs\n",
    "    bert_outputs_org = bert_qqp(bert_inputs_org, training=False)\n",
    "    \n",
    "    # get the pooled embedding and sequence embedding \n",
    "    pooled_output_org = bert_outputs_org['pooled_output'] # sentence-level embedding = pooled embedding\n",
    "    pooled_output_org_list.append(pooled_output_org)\n",
    "    sequence_output_org = bert_outputs_org['sequence_output'] # token-level embedding = per-token embedding = sequence embedding \n",
    "    sequence_output_org_list.append(sequence_output_org)\n",
    "    \n",
    "    # ======================================\n",
    "    # prompt\n",
    "    prompt_sentences = prompt_list[i]\n",
    "    \n",
    "    # convert the sentences to bert inputs\n",
    "    bert_inputs_prompt = preprocess(prompt_sentences)\n",
    "\n",
    "    # feed the inputs to the model to get the pooled and sequence outputs\n",
    "    bert_outputs_prompt = bert_qqp(bert_inputs_prompt, training=False)\n",
    "    \n",
    "    # get the pooled embedding and sequence embedding \n",
    "    pooled_output_prompt = bert_outputs_prompt['pooled_output'] # sentence-level embedding = pooled embedding\n",
    "    pooled_output_prompt_list.append(pooled_output_prompt)\n",
    "    sequence_output_prompt = bert_outputs_prompt['sequence_output'] # token-level embedding = per-token embedding = sequence embedding \n",
    "    sequence_output_prompt_list.append(sequence_output_prompt)\n",
    "    \n",
    "    # ======================================\n",
    "    # X (org + prompt)\n",
    "    X_sentences = [X_list[i].item()]\n",
    "\n",
    "    # convert the sentences to bert inputs\n",
    "    bert_inputs_X = preprocess(X_sentences)\n",
    "\n",
    "    # feed the inputs to the model to get the pooled and sequence outputs\n",
    "    bert_outputs_X = bert_qqp(bert_inputs_X, training=False)\n",
    "    \n",
    "    # get the pooled embedding and sequence embedding \n",
    "    pooled_output_X = bert_outputs_X['pooled_output'] # sentence-level embedding = pooled embedding\n",
    "    pooled_output_X_list.append(pooled_output_X)\n",
    "    sequence_output_X = bert_outputs_X['sequence_output'] # token-level embedding = per-token embedding = sequence embedding \n",
    "    sequence_output_X_list.append(sequence_output_X)\n",
    "    \n",
    "    # ======================================\n",
    "    # rewritten\n",
    "    rew_sentences = rew_text_list[i]\n",
    "    \n",
    "    # convert the sentences to bert inputs\n",
    "    bert_inputs_rew = preprocess(rew_sentences)\n",
    "\n",
    "    # feed the inputs to the model to get the pooled and sequence outputs\n",
    "    bert_outputs_rew = bert_qqp(bert_inputs_rew, training=False)\n",
    "    \n",
    "    # get the pooled embedding and sequence embedding \n",
    "    pooled_output_rew = bert_outputs_rew['pooled_output'] # sentence-level embedding = pooled embedding\n",
    "    pooled_output_rew_list.append(pooled_output_prompt)\n",
    "    sequence_output_rew = bert_outputs_rew['sequence_output'] # token-level embedding = per-token embedding = sequence embedding \n",
    "    sequence_output_rew_list.append(sequence_output_prompt)\n",
    "    \n",
    "    # ======================================\n",
    "    # ======================================\n",
    "    # compute similarity \n",
    "    cos_sim = pairwise.cosine_similarity(pooled_output_org_list[i], pooled_output_rew_list[i])\n",
    "    cos_sim_list_org_rew.append(cos_sim)\n",
    "    \n",
    "    cos_sim = pairwise.cosine_similarity(pooled_output_prompt_list[i], pooled_output_rew_list[i])\n",
    "    cos_sim_list_prompt_rew.append(cos_sim)\n",
    "    \n",
    "    cos_sim = pairwise.cosine_similarity(pooled_output_X_list[i], pooled_output_rew_list[i])\n",
    "    cos_sim_list_X_rew.append(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:51:11.513473Z",
     "iopub.status.busy": "2024-04-30T04:51:11.513053Z",
     "iopub.status.idle": "2024-04-30T04:51:11.525237Z",
     "shell.execute_reply": "2024-04-30T04:51:11.523865Z",
     "shell.execute_reply.started": "2024-04-30T04:51:11.513440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini - mean of similarity between org, rew:  [array([[297.5181]], dtype=float32), array([[0.8752649]], dtype=float32), array([[0.9845989]], dtype=float32), array([[0.94869226]], dtype=float32), array([[0.96205]], dtype=float32), array([[0.9763001]], dtype=float32), array([[0.9561322]], dtype=float32), array([[0.15212032]], dtype=float32), array([[0.12496223]], dtype=float32), array([[0.4855021]], dtype=float32)]\n",
      "\n",
      "\n",
      "gemini - mean of similarity between prompt, rew:  [array([[330.]], dtype=float32), array([[0.9999999]], dtype=float32), array([[1.0000001]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[0.9999999]], dtype=float32), array([[1.]], dtype=float32), array([[0.9999998]], dtype=float32), array([[0.9999999]], dtype=float32), array([[1.]], dtype=float32)]\n",
      "\n",
      "\n",
      "gemini - mean of similarity between org+prompt, rew:  [array([[299.89655]], dtype=float32), array([[0.8549682]], dtype=float32), array([[0.98939437]], dtype=float32), array([[0.8950499]], dtype=float32), array([[0.96901345]], dtype=float32), array([[0.9655119]], dtype=float32), array([[0.953099]], dtype=float32), array([[0.1071533]], dtype=float32), array([[0.06813136]], dtype=float32), array([[0.60057294]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# similarity mean\n",
    "print('gemini - mean of similarity between org, rew: ', cos_sim_list_org_rew[0:10])\n",
    "print('\\n')\n",
    "print('gemini - mean of similarity between prompt, rew: ', cos_sim_list_prompt_rew[0:10])\n",
    "print('\\n')\n",
    "print('gemini - mean of similarity between org+prompt, rew: ', cos_sim_list_X_rew[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:54:57.090156Z",
     "iopub.status.busy": "2024-04-30T04:54:57.089687Z",
     "iopub.status.idle": "2024-04-30T04:54:57.099167Z",
     "shell.execute_reply": "2024-04-30T04:54:57.098169Z",
     "shell.execute_reply.started": "2024-04-30T04:54:57.090120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.51406"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cos_sim_list_org_rew[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:54:55.646218Z",
     "iopub.status.busy": "2024-04-30T04:54:55.645838Z",
     "iopub.status.idle": "2024-04-30T04:54:55.656891Z",
     "shell.execute_reply": "2024-04-30T04:54:55.655246Z",
     "shell.execute_reply.started": "2024-04-30T04:54:55.646191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.735077"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cos_sim_list_prompt_rew[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:55:02.707935Z",
     "iopub.status.busy": "2024-04-30T04:55:02.707503Z",
     "iopub.status.idle": "2024-04-30T04:55:02.717885Z",
     "shell.execute_reply": "2024-04-30T04:55:02.716430Z",
     "shell.execute_reply.started": "2024-04-30T04:55:02.707902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.75065"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cos_sim_list_X_rew[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:22:34.942155Z",
     "iopub.status.busy": "2024-04-30T04:22:34.941680Z",
     "iopub.status.idle": "2024-04-30T04:42:06.387750Z",
     "shell.execute_reply": "2024-04-30T04:42:06.386468Z",
     "shell.execute_reply.started": "2024-04-30T04:22:34.942114Z"
    }
   },
   "outputs": [],
   "source": [
    "# gemma\n",
    "\n",
    "# create lists to record output embedding\n",
    "pooled_output_org_list_gemma = []\n",
    "sequence_output_org_list_gemma = []\n",
    "\n",
    "pooled_output_prompt_list_gemma = []\n",
    "sequence_output_prompt_list_gemma = []\n",
    "\n",
    "pooled_output_X_list_gemma = []\n",
    "sequence_output_X_list_gemma = []\n",
    "\n",
    "pooled_output_rew_list_gemma = []\n",
    "sequence_output_rew_list_gemma = []\n",
    "\n",
    "# lists to record similarity between lists and rewritten \n",
    "cos_sim_list_org_rew_gemma = []\n",
    "cos_sim_list_prompt_rew_gemma = []\n",
    "cos_sim_list_X_rew_gemma = []\n",
    "\n",
    "\n",
    "for i in range(num): \n",
    "    # ======================================\n",
    "    # org \n",
    "    org_sentences = org_text_list_gemma[i]\n",
    "    \n",
    "    # convert the sentences to bert inputs\n",
    "    bert_inputs_org = preprocess(org_sentences)\n",
    "    \n",
    "    # feed the inputs to the model to get the pooled and sequence outputs\n",
    "    bert_outputs_org_gemma = bert_qqp(bert_inputs_org, training=False)\n",
    "    \n",
    "    # get the pooled embedding and sequence embedding \n",
    "    pooled_output_org_gemma = bert_outputs_org_gemma['pooled_output'] # sentence-level embedding = pooled embedding\n",
    "    pooled_output_org_list_gemma.append(pooled_output_org_gemma)\n",
    "    sequence_output_org_gemma = bert_outputs_org_gemma['sequence_output'] # token-level embedding = per-token embedding = sequence embedding \n",
    "    sequence_output_org_list_gemma.append(sequence_output_org_gemma)\n",
    "    \n",
    "    # ======================================\n",
    "    # prompt\n",
    "    prompt_sentences = [prompt_list_gemma[i]]\n",
    "    \n",
    "    # convert the sentences to bert inputs\n",
    "    bert_inputs_prompt = preprocess(prompt_sentences)\n",
    "\n",
    "    # feed the inputs to the model to get the pooled and sequence outputs\n",
    "    bert_outputs_prompt = bert_qqp(bert_inputs_prompt, training=False)\n",
    "    \n",
    "    # get the pooled embedding and sequence embedding \n",
    "    pooled_output_prompt = bert_outputs_prompt['pooled_output'] # sentence-level embedding = pooled embedding\n",
    "    pooled_output_prompt_list_gemma.append(pooled_output_prompt)\n",
    "    sequence_output_prompt = bert_outputs_prompt['sequence_output'] # token-level embedding = per-token embedding = sequence embedding \n",
    "    sequence_output_prompt_list_gemma.append(sequence_output_prompt)\n",
    "    \n",
    "    # ======================================\n",
    "    # X (org + prompt)\n",
    "    X_sentences = [X_list_gemma[i].item()]\n",
    "\n",
    "    # convert the sentences to bert inputs\n",
    "    bert_inputs_X = preprocess(X_sentences)\n",
    "\n",
    "    # feed the inputs to the model to get the pooled and sequence outputs\n",
    "    bert_outputs_X = bert_qqp(bert_inputs_X, training=False)\n",
    "    \n",
    "    # get the pooled embedding and sequence embedding \n",
    "    pooled_output_X = bert_outputs_X['pooled_output'] # sentence-level embedding = pooled embedding\n",
    "    pooled_output_X_list_gemma.append(pooled_output_X)\n",
    "    sequence_output_X = bert_outputs_X['sequence_output'] # token-level embedding = per-token embedding = sequence embedding \n",
    "    sequence_output_X_list_gemma.append(sequence_output_X)\n",
    "    \n",
    "    # ======================================\n",
    "    # rewritten\n",
    "    rew_sentences = rew_text_list_gemma[i]\n",
    "    \n",
    "    # convert the sentences to bert inputs\n",
    "    bert_inputs_rew = preprocess(rew_sentences)\n",
    "\n",
    "    # feed the inputs to the model to get the pooled and sequence outputs\n",
    "    bert_outputs_rew = bert_qqp(bert_inputs_rew, training=False)\n",
    "    \n",
    "    # get the pooled embedding and sequence embedding \n",
    "    pooled_output_rew = bert_outputs_rew['pooled_output'] # sentence-level embedding = pooled embedding\n",
    "    pooled_output_rew_list_gemma.append(pooled_output_prompt)\n",
    "    sequence_output_rew = bert_outputs_rew['sequence_output'] # token-level embedding = per-token embedding = sequence embedding \n",
    "    sequence_output_rew_list_gemma.append(sequence_output_prompt)\n",
    "    \n",
    "    # ======================================\n",
    "    # ======================================\n",
    "    # compute similarity \n",
    "    cos_sim = pairwise.cosine_similarity(pooled_output_org_list_gemma[i], pooled_output_rew_list_gemma[i])\n",
    "    cos_sim_list_org_rew_gemma.append(cos_sim)\n",
    "    \n",
    "    cos_sim = pairwise.cosine_similarity(pooled_output_prompt_list_gemma[i], pooled_output_rew_list_gemma[i])\n",
    "    cos_sim_list_prompt_rew_gemma.append(cos_sim)\n",
    "    \n",
    "    cos_sim = pairwise.cosine_similarity(pooled_output_X_list_gemma[i], pooled_output_rew_list_gemma[i])\n",
    "    cos_sim_list_X_rew_gemma.append(cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:52:34.485300Z",
     "iopub.status.busy": "2024-04-30T04:52:34.484752Z",
     "iopub.status.idle": "2024-04-30T04:52:34.502804Z",
     "shell.execute_reply": "2024-04-30T04:52:34.501154Z",
     "shell.execute_reply.started": "2024-04-30T04:52:34.485249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemma - mean of similarity between org, rew:  [array([[0.77171415]], dtype=float32), array([[0.9896351]], dtype=float32), array([[0.9870037]], dtype=float32), array([[0.9735609]], dtype=float32), array([[0.99080646]], dtype=float32), array([[0.8313054]], dtype=float32), array([[0.8892137]], dtype=float32), array([[0.91462356]], dtype=float32), array([[0.9869556]], dtype=float32), array([[0.9705299]], dtype=float32)]\n",
      "gemma - mean of similarity between prompt, rew:  [array([[1.0000001]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[0.9999998]], dtype=float32), array([[0.99999976]], dtype=float32), array([[0.99999994]], dtype=float32), array([[0.99999976]], dtype=float32), array([[0.9999999]], dtype=float32), array([[0.9999999]], dtype=float32), array([[1.]], dtype=float32)]\n",
      "gemma - mean of similarity between org+prompt, rew:  [array([[0.7717141]], dtype=float32), array([[0.9896353]], dtype=float32), array([[0.98700386]], dtype=float32), array([[0.9735608]], dtype=float32), array([[0.99080646]], dtype=float32), array([[0.8313054]], dtype=float32), array([[0.8892137]], dtype=float32), array([[0.91462356]], dtype=float32), array([[0.9869556]], dtype=float32), array([[0.9705299]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# similarity mean\n",
    "print('gemma - mean of similarity between org, rew: ', cos_sim_list_org_rew_gemma[0:10])\n",
    "print('gemma - mean of similarity between prompt, rew: ', cos_sim_list_prompt_rew_gemma[0:10])\n",
    "print('gemma - mean of similarity between org+prompt, rew: ', cos_sim_list_X_rew_gemma[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:54:30.984886Z",
     "iopub.status.busy": "2024-04-30T04:54:30.984515Z",
     "iopub.status.idle": "2024-04-30T04:54:30.994435Z",
     "shell.execute_reply": "2024-04-30T04:54:30.992773Z",
     "shell.execute_reply.started": "2024-04-30T04:54:30.984858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11395453"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cos_sim_list_org_rew_gemma[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:54:29.346192Z",
     "iopub.status.busy": "2024-04-30T04:54:29.345811Z",
     "iopub.status.idle": "2024-04-30T04:54:29.357165Z",
     "shell.execute_reply": "2024-04-30T04:54:29.355550Z",
     "shell.execute_reply.started": "2024-04-30T04:54:29.346163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4781472e-07"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cos_sim_list_prompt_rew_gemma[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T04:54:27.089277Z",
     "iopub.status.busy": "2024-04-30T04:54:27.088896Z",
     "iopub.status.idle": "2024-04-30T04:54:27.099040Z",
     "shell.execute_reply": "2024-04-30T04:54:27.097951Z",
     "shell.execute_reply.started": "2024-04-30T04:54:27.089249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4781472e-07"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(cos_sim_list_prompt_rew_gemma[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, there are not much difference in mean of similarity.   \n",
    "However, in terms of standard deviation, gemma model has an overall smaller sd, indicating more stable performance. "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4524732,
     "sourceId": 7742875,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 185,
     "sourceId": 281,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 190,
     "sourceId": 291,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2180,
     "sourceId": 2938,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
